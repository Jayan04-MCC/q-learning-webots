# Proyecto Q-Learning: NavegaciÃ³n AutÃ³noma con Robot E-puck

ImplementaciÃ³n de Q-Learning para navegaciÃ³n autÃ³noma de un robot E-puck que aprende a alcanzar un objetivo verde evitando obstÃ¡culos en Webots.

**Autor**: Jayan Caceres Cuba

---

## Tabla de Contenidos

- [Evidencia de Funcionamiento](#evidencia-de-funcionamiento)
- [Arquitectura del Sistema](#arquitectura-del-sistema)
- [Componentes Esenciales del CÃ³digo](#componentes-esenciales-del-cÃ³digo)
- [Sistema de Recompensas](#sistema-de-recompensas)
- [Ciclo de Aprendizaje](#ciclo-de-aprendizaje)
- [ParÃ¡metros Configurables](#parÃ¡metros-configurables)
- [EjecuciÃ³n del Proyecto](#ejecuciÃ³n-del-proyecto)

---

## Evidencia de Funcionamiento

<!-- Insertar aquÃ­ captura de pantalla o GIF del robot navegando -->

![alt text](image-1.png)

**DescripciÃ³n**: Robot E-puck navegando desde la posiciÃ³n inicial (-0.7, -0.7) hasta el objetivo verde en (0.7, 0.7) tras N episodios de entrenamiento.

---

## Arquitectura del Sistema

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      ENTORNO WEBOTS                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Arena 2x2m                                          â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”         WALL1         â”Œâ”€â”€â”€â”€â”€â”              â”‚   â”‚
â”‚  â”‚  â”‚     â”‚           â•‘            â”‚     â”‚              â”‚   â”‚
â”‚  â”‚  â”‚  E  â”‚    WALL2â•â•â•£            â”‚  ğŸŸ¢ â”‚ Objetivo     â”‚   â”‚
â”‚  â”‚  â”‚puck â”‚           â•‘            â”‚Verdeâ”‚ (0.7,0.7)    â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”˜                        â””â”€â”€â”€â”€â”€â”˜              â”‚   â”‚
â”‚  â”‚  (-0.7,-0.7)                                         â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“â†‘
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   Q-LEARNING CONTROLLER (Python)      â”‚
        â”‚                                       â”‚
        â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
        â”‚  â”‚  1. PercepciÃ³n                  â”‚ â”‚
        â”‚  â”‚     â€¢ 8 Sensores de distancia   â”‚ â”‚
        â”‚  â”‚     â€¢ CÃ¡mara 64x48 (detecciÃ³n)  â”‚ â”‚
        â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
        â”‚              â†“                        â”‚
        â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
        â”‚  â”‚  2. DiscretizaciÃ³n de Estado    â”‚ â”‚
        â”‚  â”‚     â€¢ 4 Direcciones Ã— 3 Niveles â”‚ â”‚
        â”‚  â”‚     â€¢ Estado = (F,L,R,A)        â”‚ â”‚
        â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
        â”‚              â†“                        â”‚
        â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
        â”‚  â”‚  3. DecisiÃ³n (Epsilon-Greedy)   â”‚ â”‚
        â”‚  â”‚     â€¢ ExploraciÃ³n vs ExplotaciÃ³nâ”‚ â”‚
        â”‚  â”‚     â€¢ Q-Table lookup            â”‚ â”‚
        â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
        â”‚              â†“                        â”‚
        â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
        â”‚  â”‚  4. AcciÃ³n                      â”‚ â”‚
        â”‚  â”‚     â€¢ Forward / Left / Right    â”‚ â”‚
        â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
        â”‚              â†“                        â”‚
        â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
        â”‚  â”‚  5. Recompensa                  â”‚ â”‚
        â”‚  â”‚     â€¢ +100: Objetivo alcanzado  â”‚ â”‚
        â”‚  â”‚     â€¢ -10: ColisiÃ³n             â”‚ â”‚
        â”‚  â”‚     â€¢ +0~1.5: Ver verde         â”‚ â”‚
        â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
        â”‚              â†“                        â”‚
        â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
        â”‚  â”‚  6. ActualizaciÃ³n Q-Table       â”‚ â”‚
        â”‚  â”‚     â€¢ Q(s,a) â† Q(s,a) + Î±(...)  â”‚ â”‚
        â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Componentes Esenciales del CÃ³digo

### 1. InicializaciÃ³n del Sistema
**Archivo**: `controllers/q_learning_controller/q_learning_controller.py`

```python
# LÃ­neas 14-79: Constructor de la clase
class QLearningRobot:
    def __init__(self):
        # ParÃ¡metros de Q-Learning (lÃ­neas 21-25)
        self.learning_rate = 0.1       # Î± - Tasa de aprendizaje
        self.discount_factor = 0.9     # Î³ - Factor de descuento
        self.epsilon = 1.0             # ExploraciÃ³n inicial (100%)
        self.epsilon_decay = 0.995     # Decaimiento por episodio
        self.epsilon_min = 0.01        # ExploraciÃ³n mÃ­nima (1%)

        # LÃ­mites de episodio (lÃ­neas 62-64)
        self.max_steps_per_episode = 1000  # 16 segundos mÃ¡ximo

        # Tabla Q (lÃ­nea 28)
        self.q_table = {}  # Diccionario: {estado: [Q(s,a1), Q(s,a2), Q(s,a3)]}
```

**FunciÃ³n**: Configura los hiperparÃ¡metros del algoritmo y inicializa los dispositivos del robot.

---

### 2. PercepciÃ³n: Lectura de Sensores
**LÃ­neas 99-108**

```python
def get_sensor_readings(self):
    """Obtener lecturas normalizadas de 8 sensores de distancia"""
    readings = []
    for sensor in self.distance_sensors:
        value = sensor.getValue()
        # Normalizar: 0 = lejos, 1 = muy cerca
        normalized = min(value / 4096.0, 1.0)
        readings.append(normalized)
    return readings
```

**FunciÃ³n**: Lee los 8 sensores infrarrojos y normaliza sus valores al rango [0, 1].

---

### 3. DiscretizaciÃ³n del Estado
**LÃ­neas 110-130**

```python
def discretize_state(self, sensor_readings):
    """Agrupa 8 sensores en 4 direcciones con 3 niveles cada una"""
    # Agrupar sensores por direcciÃ³n
    front = max(sensor_readings[0], sensor_readings[7])  # ps0, ps7
    left = max(sensor_readings[5], sensor_readings[6])   # ps5, ps6
    right = max(sensor_readings[1], sensor_readings[2])  # ps1, ps2
    back = max(sensor_readings[3], sensor_readings[4])   # ps3, ps4

    # Discretizar cada direcciÃ³n en 3 niveles
    def discretize_value(value):
        if value < 0.15:
            return 0  # Libre
        elif value < 0.40:
            return 1  # Cerca
        else:
            return 2  # Muy cerca

    state = (discretize_value(front),
             discretize_value(left),
             discretize_value(right),
             discretize_value(back))

    return state  # Tupla: (F, L, R, A)
```

**FunciÃ³n**: Convierte las 8 lecturas continuas en un estado discreto de 4 dimensiones con 3 niveles cada una.
**Espacio de estados**: 3^4 = 81 estados posibles.

---

### 4. DetecciÃ³n del Objetivo (VisiÃ³n)
**LÃ­neas 170-205**

```python
def detect_goal(self):
    """Detecta pÃ­xeles verdes en la imagen de la cÃ¡mara"""
    if not self.camera:
        return 0.0

    image = self.camera.getImage()
    if not image:
        return 0.0

    width = self.camera.getWidth()   # 64 pÃ­xeles
    height = self.camera.getHeight()  # 48 pÃ­xeles

    green_pixels = 0
    total_pixels = width * height

    # Analizar cada pÃ­xel
    for y in range(height):
        for x in range(width):
            # Obtener componentes RGB
            r = self.camera.imageGetRed(image, width, x, y)
            g = self.camera.imageGetGreen(image, width, x, y)
            b = self.camera.imageGetBlue(image, width, x, y)

            # Criterio de detecciÃ³n de verde
            if g > 150 and g > r * 1.5 and g > b * 1.5:
                green_pixels += 1

    # Retornar proporciÃ³n de verde [0.0, 1.0]
    return green_pixels / total_pixels
```

**FunciÃ³n**: Analiza los 3,072 pÃ­xeles de la cÃ¡mara y calcula el porcentaje que es verde.

---

### 5. CÃ¡lculo de Recompensas
**LÃ­neas 207-230**

```python
def calculate_reward(self, sensor_readings):
    """Sistema de recompensas basado en sensores y visiÃ³n"""

    # 1. Detectar objetivo verde
    green_ratio = self.detect_goal()

    # 2. RECOMPENSA MÃXIMA: Objetivo alcanzado
    if green_ratio > self.goal_detection_threshold:  # >30% verde
        return 100.0, True  # Episodio terminado

    # 3. PENALIZACIÃ“N: ColisiÃ³n con obstÃ¡culo
    max_sensor = max(sensor_readings)
    if max_sensor > 0.5:  # ObstÃ¡culo muy cerca
        return -10.0, False

    # 4. RECOMPENSA PROPORCIONAL: Ver algo de verde
    # FÃ³rmula: -0.01 (costo por paso) + 5 Ã— (% verde)
    reward = -0.01 + (green_ratio * 5.0)

    return reward, False
```

**Tabla de Recompensas**:
| CondiciÃ³n | Valor | Efecto |
|-----------|-------|--------|
| `green_ratio > 0.30` | **+100.0** | Termina episodio |
| `max_sensor > 0.50` | **-10.0** | ContinÃºa |
| `0 < green_ratio â‰¤ 0.30` | `-0.01 + (green_ratio Ã— 5.0)` | GuÃ­a hacia objetivo |

---

### 6. DecisiÃ³n de AcciÃ³n (Epsilon-Greedy)
**LÃ­neas 132-158**

```python
def choose_action(self, state):
    """Estrategia Epsilon-Greedy para balancear exploraciÃ³n/explotaciÃ³n"""

    # ExploraciÃ³n: AcciÃ³n aleatoria
    if random.random() < self.epsilon:
        return random.randint(0, self.num_actions - 1)

    # ExplotaciÃ³n: Mejor acciÃ³n segÃºn Q-Table
    q_values = self.get_q_values(state)
    return np.argmax(q_values)

def get_q_values(self, state):
    """Obtiene o inicializa Q-values para un estado"""
    if state not in self.q_table:
        # Inicializar con ceros
        self.q_table[state] = np.zeros(self.num_actions)
    return self.q_table[state]
```

**FunciÃ³n**: Decide entre explorar (acciÃ³n aleatoria) o explotar (mejor acciÃ³n conocida).

---

### 7. EjecuciÃ³n de Acciones
**LÃ­neas 160-168**

```python
def execute_action(self, action_index):
    """Traduce Ã­ndice de acciÃ³n a comandos de motores"""
    action = self.actions[action_index]

    if action == 'forward':
        self.left_motor.setVelocity(self.max_speed)
        self.right_motor.setVelocity(self.max_speed)
    elif action == 'left':
        self.left_motor.setVelocity(-self.max_speed * 0.5)
        self.right_motor.setVelocity(self.max_speed * 0.5)
    elif action == 'right':
        self.left_motor.setVelocity(self.max_speed * 0.5)
        self.right_motor.setVelocity(-self.max_speed * 0.5)
```

**Acciones disponibles**:
- `0 = forward`: Avanzar recto
- `1 = left`: Girar a la izquierda
- `2 = right`: Girar a la derecha

---

### 8. ActualizaciÃ³n de Q-Values
**LÃ­neas 232-239**

```python
def update_q_value(self, state, action, reward, next_state):
    """Aplica la ecuaciÃ³n de Bellman para Q-Learning"""

    # Obtener Q-value actual
    current_q = self.get_q_values(state)[action]

    # Obtener mejor Q-value del siguiente estado
    next_max_q = np.max(self.get_q_values(next_state))

    # EcuaciÃ³n de Q-Learning
    new_q = current_q + self.learning_rate * (
        reward + self.discount_factor * next_max_q - current_q
    )

    # Actualizar tabla Q
    self.q_table[state][action] = new_q
```

**EcuaciÃ³n**:
```
Q(s,a) â† Q(s,a) + Î± Ã— [r + Î³ Ã— max(Q(s',a')) - Q(s,a)]
```

**Donde**:
- `Q(s,a)`: Valor actual de tomar acciÃ³n `a` en estado `s`
- `Î± = 0.1`: Tasa de aprendizaje
- `r`: Recompensa inmediata
- `Î³ = 0.9`: Factor de descuento (importancia del futuro)
- `max(Q(s',a'))`: Mejor valor en el siguiente estado

---

### 9. Ciclo Principal de Aprendizaje
**LÃ­neas 268-307**

```python
def run(self):
    """Bucle principal de entrenamiento"""

    # Obtener estado inicial
    sensor_readings = self.get_sensor_readings()
    self.current_state = self.discretize_state(sensor_readings)

    while self.robot.step(self.timestep) != -1:
        # PASO 1: Elegir acciÃ³n
        self.current_action = self.choose_action(self.current_state)

        # PASO 2: Ejecutar acciÃ³n
        self.execute_action(self.current_action)
        self.robot.step(self.timestep)

        # PASO 3: Observar resultado
        sensor_readings = self.get_sensor_readings()
        next_state = self.discretize_state(sensor_readings)
        reward, done = self.calculate_reward(sensor_readings)

        # PASO 4: Aprender (actualizar Q-Table)
        self.update_q_value(self.current_state, self.current_action,
                           reward, next_state)

        # PASO 5: Actualizar estado
        self.current_state = next_state
        self.total_reward += reward
        self.steps += 1

        # PASO 6: Verificar fin de episodio
        if done or self.steps >= self.max_steps_per_episode:
            print(f"Episodio {self.episode}, Pasos: {self.steps}, "
                  f"Recompensa: {self.total_reward:.2f}")
            self.reset_robot()
```

**Flujo de ejecuciÃ³n**:
```
Iniciar â†’ [Observar estado] â†’ [Elegir acciÃ³n] â†’ [Ejecutar] â†’
[Recibir recompensa] â†’ [Actualizar Q-Table] â†’ Â¿TerminÃ³?
                                                 â”œâ”€ No â†’ Repetir
                                                 â””â”€ SÃ­ â†’ Reiniciar episodio
```

---

### 10. Reinicio de Episodio
**LÃ­neas 241-266**

```python
def reset_robot(self):
    """Prepara el siguiente episodio de entrenamiento"""

    self.episode += 1
    self.steps = 0
    self.total_reward = 0

    # Reducir epsilon (menos exploraciÃ³n con el tiempo)
    self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)

    # Guardar progreso cada 10 episodios
    if self.episode % 10 == 0:
        self.save_q_table()
        print(f"Epsilon: {self.epsilon:.3f} - Estados: {len(self.q_table)}")

    # Reiniciar posiciÃ³n fÃ­sica
    translation_field = self.robot_node.getField('translation')
    rotation_field = self.robot_node.getField('rotation')
    translation_field.setSFVec3f(self.initial_translation)
    rotation_field.setSFRotation(self.initial_rotation)
    self.robot_node.resetPhysics()
```

**FunciÃ³n**: Reinicia el robot a su posiciÃ³n inicial y actualiza epsilon.

---

### 11. Persistencia de Datos
**LÃ­neas 80-97**

```python
def load_q_table(self):
    """Cargar tabla Q desde archivo al iniciar"""
    if os.path.exists(self.q_table_file):
        with open(self.q_table_file, 'rb') as f:
            self.q_table = pickle.load(f)
        print(f"Tabla Q cargada: {len(self.q_table)} estados")

def save_q_table(self):
    """Guardar tabla Q en archivo"""
    with open(self.q_table_file, 'wb') as f:
        pickle.dump(self.q_table, f)
    print(f"Tabla Q guardada: {len(self.q_table)} estados")
```

**FunciÃ³n**: Permite continuar el entrenamiento entre ejecuciones.

---

## Sistema de Recompensas

### Diagrama de Flujo

```
                    Ejecutar AcciÃ³n
                          â†“
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚  Leer Sensores + CÃ¡mara â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚  Â¿green_ratio > 0.30?   â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“          â†“
                  SÃ           NO
                   â†“            â†“
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚ Recompensa: â”‚  â”‚ Â¿max_sensor    â”‚
         â”‚   +100.0    â”‚  â”‚   > 0.50?      â”‚
         â”‚ Episodio    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ TERMINA     â”‚       â†“        â†“
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      SÃ        NO
                              â†“          â†“
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ Recompensa:  â”‚  â”‚ Recompensa:       â”‚
                    â”‚   -10.0      â”‚  â”‚ -0.01 + (greenÃ—5) â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Tabla Detallada

| Evento | CondiciÃ³n | Recompensa | LÃ­nea | Termina Episodio |
|--------|-----------|-----------|-------|------------------|
| **Objetivo alcanzado** | `green_ratio > 0.30` | `+100.0` | 219-220 | âœ… SÃ­ |
| **ColisiÃ³n inminente** | `max_sensor > 0.50` | `-10.0` | 223-225 | âŒ No |
| **Ver objetivo (30%)** | `green_ratio = 0.30` | `+1.49` | 228 | âŒ No |
| **Ver objetivo (20%)** | `green_ratio = 0.20` | `+0.99` | 228 | âŒ No |
| **Ver objetivo (10%)** | `green_ratio = 0.10` | `+0.49` | 228 | âŒ No |
| **Sin objetivo visible** | `green_ratio = 0` | `-0.01` | 228 | âŒ No |

---

## Ciclo de Aprendizaje

### ProgresiÃ³n de Epsilon

```
Episodio 0:    Îµ = 1.000  (100% exploraciÃ³n)
Episodio 10:   Îµ = 0.951  (95% exploraciÃ³n)
Episodio 50:   Îµ = 0.778  (78% exploraciÃ³n)
Episodio 100:  Îµ = 0.606  (61% exploraciÃ³n)
Episodio 200:  Îµ = 0.367  (37% exploraciÃ³n)
Episodio 500:  Îµ = 0.081  (8% exploraciÃ³n)
Episodio 1000: Îµ = 0.010  (1% exploraciÃ³n) â† MÃ­nimo
```

### EvoluciÃ³n TÃ­pica del Aprendizaje

**Fase 1: ExploraciÃ³n CaÃ³tica (Episodios 1-100)**
- Comportamiento aleatorio predominante
- Muchas colisiones con obstÃ¡culos
- Recompensas totales negativas
- Tabla Q crece rÃ¡pidamente (descubrimiento)

**Fase 2: Aprendizaje Activo (Episodios 100-300)**
- Balance exploraciÃ³n/explotaciÃ³n
- Comienza a evitar obstÃ¡culos
- Ocasionalmente alcanza el objetivo
- Recompensas totales aumentan gradualmente

**Fase 3: Refinamiento (Episodios 300+)**
- Comportamiento mayormente explotativo
- Rutas eficientes y consistentes
- Alta tasa de Ã©xito en alcanzar objetivo
- NÃºmero de pasos disminuye

---

## ParÃ¡metros Configurables

### Archivo: `q_learning_controller.py`

| ParÃ¡metro | LÃ­nea | Valor Default | DescripciÃ³n | Efecto al Aumentar |
|-----------|-------|---------------|-------------|-------------------|
| `learning_rate` | 21 | `0.1` | Velocidad de actualizaciÃ³n de Q-values | Aprende mÃ¡s rÃ¡pido pero menos estable |
| `discount_factor` | 22 | `0.9` | Importancia de recompensas futuras | Mayor planificaciÃ³n a largo plazo |
| `epsilon` | 23 | `1.0` | ExploraciÃ³n inicial | Mayor aleatoriedad al inicio |
| `epsilon_decay` | 24 | `0.995` | Velocidad de reducciÃ³n de Îµ | Reduce exploraciÃ³n mÃ¡s lentamente |
| `epsilon_min` | 25 | `0.01` | ExploraciÃ³n mÃ­nima | Mantiene mÃ¡s exploraciÃ³n siempre |
| `max_steps_per_episode` | 64 | `1000` | Pasos antes de timeout (16s) | Da mÃ¡s tiempo para encontrar objetivo |
| `goal_detection_threshold` | 59 | `0.3` | % verde para declarar Ã©xito | Requiere estar mÃ¡s cerca del objetivo |
| `max_speed` | 43 | `6.28` | Velocidad mÃ¡xima (rad/s) | Robot mÃ¡s rÃ¡pido |

### Archivo: `proyecto-q-learning.wbt`

| ParÃ¡metro | LÃ­nea | Valor | DescripciÃ³n |
|-----------|-------|-------|-------------|
| `basicTimeStep` | 8 | `16` | Milisegundos por step de simulaciÃ³n |
| Robot translation | 12 | `[-0.7, -0.7, 0]` | PosiciÃ³n inicial |
| Objetivo translation | 33 | `[0.7, 0.7, 0.05]` | PosiciÃ³n del objetivo |
| Camera fieldOfView | 26 | `1.0` | Ãngulo de visiÃ³n (radianes) |
| Camera width | 27 | `64` | Ancho de imagen (pÃ­xeles) |
| Camera height | 28 | `48` | Alto de imagen (pÃ­xeles) |

---

## EjecuciÃ³n del Proyecto

### Requisitos

- **Webots R2025a** o superior
- **Python 3.8+** (incluido con Webots)
- LibrerÃ­as: `numpy`, `pickle` (incluidas en Webots)

### Comandos

```bash
# 1. Abrir Webots
# 2. Cargar mundo
File > Open World > D:\proyectos_webots\q-learning\worlds\proyecto-q-learning.wbt

# 3. Iniciar simulaciÃ³n
Play â–¶ï¸

# 4. Observar consola
Iniciando Q-Learning...
ParÃ¡metros: Î±=0.1, Î³=0.9, Îµ=1.0
Episodio 0 terminado (max pasos), Recompensa total: -10.45
...

# 5. Para reiniciar desde cero
# Eliminar archivo: q_table.pkl
```

### Salida Esperada

```
Iniciando Q-Learning...
ParÃ¡metros: Î±=0.1, Î³=0.9, Îµ=1.0
Episodio 0 terminado (max pasos), Recompensa total: -12.34
Episodio 1 terminado (max pasos), Recompensa total: -8.76
Episodio 2 terminado (max pasos), Recompensa total: -15.23
...
Episodio 10 - Epsilon: 0.951 - Estados aprendidos: 45
...
Â¡Objetivo alcanzado! Episodio 23, Pasos: 687, Recompensa total: 78.45
...
Episodio 100 - Epsilon: 0.606 - Estados aprendidos: 81
Â¡Objetivo alcanzado! Episodio 104, Pasos: 342, Recompensa total: 93.12
```

---

